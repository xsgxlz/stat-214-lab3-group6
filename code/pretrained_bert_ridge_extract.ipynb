{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('code')\n",
    "sys.path.append(\"/jet/home/azhang19/stat 214/stat-214-lab3-group6/code\")\n",
    "\n",
    "from BERT.data import TextDataset\n",
    "from finetune_bert_utils import get_sliding_window_embeddings, aggregate_embeddings, downsample_word_vectors_torch, load_fmri_data, get_fmri_data\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define the base path for data access\n",
    "data_path = '/ocean/projects/mth240012p/shared/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_101773/1482417774.py:3: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  wordseqs = pickle.load(file) # wordseqs is expected to be a dictionary: {story_id: WordSequenceObject}\n"
     ]
    }
   ],
   "source": [
    "# %% Load preprocessed word sequences (likely includes words and their timings)\n",
    "with open(f'{data_path}/raw_text.pkl', 'rb') as file:\n",
    "    wordseqs = pickle.load(file) # wordseqs is expected to be a dictionary: {story_id: WordSequenceObject}\n",
    "\n",
    "# %% Get list of story identifiers and split into training and testing sets\n",
    "# Assumes story data for 'subject2' exists and filenames are story IDs + '.npy'\n",
    "stories = [i[:-4] for i in os.listdir(f'{data_path}/subject2')] # Extract story IDs from filenames\n",
    "# Split stories into train and test sets with a fixed random state for reproducibility\n",
    "\n",
    "\n",
    "# First, use 60% for training and 40% for the remaining data.\n",
    "train_stories, temp_stories = train_test_split(stories, train_size=0.6, random_state=214)\n",
    "# Then split the remaining 40% equally to get 20% validation and 20% test.\n",
    "val_stories, test_stories = train_test_split(temp_stories, train_size=0.5, random_state=214)\n",
    "\n",
    "story_name_to_idx = {story: i for i, story in enumerate(stories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "base_model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [\" \".join(wordseqs[i].data).strip() for i in train_stories]\n",
    "train_dataset = TextDataset(train_text, tokenizer, max_len=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_range = (5, -10)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "embeddings = {}\n",
    "\n",
    "texts = []\n",
    "\n",
    "for story in stories:\n",
    "    words = wordseqs[story].data\n",
    "    texts.append(\" \".join(words).strip())\n",
    "    tokens = tokenizer(words, add_special_tokens=False, truncation=False, max_length=sys.maxsize)['input_ids']\n",
    "    token_per_word = [len(i) for i in tokens]\n",
    "tokenlized_stories = tokenizer(texts, add_special_tokens=False, padding=\"longest\", truncation=False, max_length=sys.maxsize,\n",
    "                               return_token_type_ids=False, return_tensors=\"pt\")\n",
    "input_ids = tokenlized_stories[\"input_ids\"].to(device)\n",
    "attention_mask = tokenlized_stories[\"attention_mask\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(current_stories):\n",
    "    idx = [story_name_to_idx[story] for story in current_stories]\n",
    "    embeddings = get_sliding_window_embeddings(base_model, input_ids[idx], attention_mask[idx])\n",
    "\n",
    "    features = {}\n",
    "    for i, story in enumerate(current_stories):\n",
    "        words = wordseqs[story].data\n",
    "        tokens = tokenizer(words, add_special_tokens=False, truncation=False, max_length=sys.maxsize)['input_ids']\n",
    "        token_per_word = [len(i) for i in tokens]\n",
    "        story_embeddings = embeddings[i]\n",
    "        word_embeddings = []\n",
    "        start = 0\n",
    "        for i in token_per_word:\n",
    "            end = start + i\n",
    "            if i != 0:\n",
    "                word_embedding = story_embeddings[start:end].mean(dim=0)\n",
    "            else:\n",
    "                word_embedding = torch.zeros(story_embeddings.size(1), device=device)\n",
    "            word_embeddings.append(word_embedding)\n",
    "            start = end\n",
    "        \n",
    "        features[story] = torch.stack(word_embeddings)#.cpu().numpy()\n",
    "\n",
    "    features = downsample_word_vectors_torch(current_stories, features, wordseqs)\n",
    "    for story in current_stories:\n",
    "        features[story] = features[story][trim_range[0]:trim_range[1]]\n",
    "\n",
    "    aggregated_features = aggregate_embeddings(features, current_stories)\n",
    "    return aggregated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_data = load_fmri_data(stories, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 1e-2\n",
    "classifiers = torch.load(f'/ocean/projects/mth240012p/azhang19/lab3/classifier_ckpts/best_classifiers{weight_decay}.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20263, 94251])\n",
      "torch.Size([20263, 95556])\n",
      "(20263, 94251)\n",
      "(20263, 95556)\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    features = forward_pass(train_stories)\n",
    "    pred_fmri = {}\n",
    "    true_fmri = {}\n",
    "    for subj in fmri_data.keys():\n",
    "        pred_fmri[subj] = classifiers[subj](features)\n",
    "        true_fmri[subj] = get_fmri_data(train_stories, fmri_data)[subj]\n",
    "    print(pred_fmri['subject2'].shape)\n",
    "    print(pred_fmri['subject3'].shape)\n",
    "    print(true_fmri['subject2'].shape)\n",
    "    print(true_fmri['subject3'].shape)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
