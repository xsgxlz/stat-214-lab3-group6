{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ac60aa-13f6-468d-b029-96f4a96323bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('code')\n",
    "sys.path.append(\"/jet/home/azhang19/stat 214/stat-214-lab3-group6/code\")\n",
    "\n",
    "from BERT.data import TextDataset\n",
    "from finetune_bert_utils import get_sliding_window_embeddings, aggregate_embeddings, downsample_word_vectors_torch, load_fmri_data, get_fmri_data\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define the base path for data access\n",
    "data_path = '/ocean/projects/mth240012p/shared/data'\n",
    "\n",
    "# my additions\n",
    "import shap\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b793fc1-78d2-4990-b4bd-dc87fea3ee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Load preprocessed word sequences (likely includes words and their timings)\n",
    "with open(f'{data_path}/raw_text.pkl', 'rb') as file:\n",
    "    wordseqs = pickle.load(file) # wordseqs is expected to be a dictionary: {story_id: WordSequenceObject}\n",
    "\n",
    "# %% Get list of story identifiers and split into training and testing sets\n",
    "# Assumes story data for 'subject2' exists and filenames are story IDs + '.npy'\n",
    "stories = [i[:-4] for i in os.listdir(f'{data_path}/subject2')] # Extract story IDs from filenames\n",
    "# Split stories into train and test sets with a fixed random state for reproducibility\n",
    "\n",
    "\n",
    "# First, use 60% for training and 40% for the remaining data.\n",
    "train_stories, temp_stories = train_test_split(stories, train_size=0.6, random_state=214)\n",
    "# Then split the remaining 40% equally to get 20% validation and 20% test.\n",
    "val_stories, test_stories = train_test_split(temp_stories, train_size=0.5, random_state=214)\n",
    "\n",
    "story_name_to_idx = {story: i for i, story in enumerate(stories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb8f0a4c-f5ad-4013-9536-44b07b5e7c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "base_model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48efaf95-1caa-43ce-ae27-3141fc8092a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [\" \".join(wordseqs[i].data).strip() for i in train_stories]\n",
    "train_dataset = TextDataset(train_text, tokenizer, max_len=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "551c6667-3b97-4a5e-a1c2-08d4eba4ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_range = (5, -10)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "embeddings = {}\n",
    "\n",
    "texts = []\n",
    "\n",
    "for story in stories:\n",
    "    words = wordseqs[story].data\n",
    "    texts.append(\" \".join(words).strip())\n",
    "    tokens = tokenizer(words, add_special_tokens=False, truncation=False, max_length=sys.maxsize)['input_ids']\n",
    "    token_per_word = [len(i) for i in tokens]\n",
    "tokenlized_stories = tokenizer(texts, add_special_tokens=False, padding=\"longest\", truncation=False, max_length=sys.maxsize,\n",
    "                               return_token_type_ids=False, return_tensors=\"pt\")\n",
    "input_ids = tokenlized_stories[\"input_ids\"].to(device)\n",
    "attention_mask = tokenlized_stories[\"attention_mask\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8e063ef-7bfb-4ec2-b211-f5a4a6151597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(current_stories, base_model):\n",
    "\n",
    "    idx = torch.tensor([story_name_to_idx[story] for story in current_stories], device=input_ids.device)\n",
    "    selected_input_ids = input_ids[idx].to(input_ids.device)\n",
    "    selected_attention_mask = attention_mask[idx].to(attention_mask.device)\n",
    "    \n",
    "    #idx = [story_name_to_idx[story] for story in current_stories]\n",
    "    #embeddings = get_sliding_window_embeddings(base_model, input_ids[idx], attention_mask[idx])\n",
    "\n",
    "    print(base_model.device)\n",
    "    print(selected_input_ids.device)\n",
    "    print(selected_attention_mask.device)\n",
    "\n",
    "    embeddings = get_sliding_window_embeddings(base_model, selected_input_ids, selected_attention_mask)\n",
    "\n",
    "    features = {}\n",
    "    for i, story in enumerate(current_stories):\n",
    "        words = wordseqs[story].data\n",
    "        tokens = tokenizer(words, add_special_tokens=False, truncation=False, max_length=sys.maxsize)['input_ids']\n",
    "        token_per_word = [len(i) for i in tokens]\n",
    "        story_embeddings = embeddings[i]\n",
    "        word_embeddings = []\n",
    "        start = 0\n",
    "        for i in token_per_word:\n",
    "            end = start + i\n",
    "            if i != 0:\n",
    "                word_embedding = story_embeddings[start:end].mean(dim=0)\n",
    "            else:\n",
    "                word_embedding = torch.zeros(story_embeddings.size(1), device=device)\n",
    "            word_embeddings.append(word_embedding)\n",
    "            start = end\n",
    "        \n",
    "        features[story] = torch.stack(word_embeddings)#.cpu().numpy()\n",
    "\n",
    "    features = downsample_word_vectors_torch(current_stories, features, wordseqs)\n",
    "    for story in current_stories:\n",
    "        features[story] = features[story][trim_range[0]:trim_range[1]]\n",
    "\n",
    "    aggregated_features = aggregate_embeddings(features, current_stories)\n",
    "    return aggregated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03d51975-4517-4f8f-957e-396bdf80a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_data = load_fmri_data(test_stories, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0486a29d-3e50-43f6-ae7a-52c278dbfcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 1e-2\n",
    "\n",
    "lora_model = True\n",
    "lora_rank = 8\n",
    "\n",
    "if lora_model:\n",
    "    config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_rank * 2,\n",
    "    target_modules=['query', 'value'],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.FEATURE_EXTRACTION\n",
    "    )\n",
    "    ckpt = torch.load(f'/jet/home/azhang19/stat 214/stat-214-lab3-group6/code/classifier_ckpt/best_lora_wd{weight_decay}_r{lora_rank}.pth', weights_only=False)\n",
    "    # was /ocean/projects/mth240012p/azhang19/lab3/classifier_ckpts\n",
    "    classifiers = {i: ckpt[i]['classifier_module'] for i in ckpt.keys()}\n",
    "    lora_weights = {i: ckpt[i]['lora_state_dict'] for i in ckpt.keys()}\n",
    "else:\n",
    "    classifiers = torch.load(f'/jet/home/azhang19/stat 214/stat-214-lab3-group6/code/classifier_ckpt/best_classifiers{weight_decay}.pth', weights_only=False)\n",
    "    # was /ocean/projects/mth240012p/azhang19/lab3/classifier_ckpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9bda946-12da-4ac5-9c7e-a2e081a576be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(pred_stories):\n",
    "    with torch.inference_mode():\n",
    "        pred_fmri = {}\n",
    "        true_fmri = {}\n",
    "        for subj in fmri_data.keys():\n",
    "            base_model = BertModel.from_pretrained(model_name).to(device).eval()\n",
    "            if lora_model:\n",
    "                base_model = get_peft_model(base_model, config).to(device)\n",
    "                base_model.load_state_dict(lora_weights[subj])\n",
    "                base_model.eval()\n",
    "        \n",
    "            features = forward_pass(pred_stories, base_model)\n",
    "            # added base_model as parameter to avoid cpu/cuda device mismatch\n",
    "            pred_fmri[subj] = classifiers[subj](features)\n",
    "            true_fmri[subj] = get_fmri_data(pred_stories, fmri_data)[subj]\n",
    "        #print(pred_fmri['subject2'].shape)\n",
    "        #print(pred_fmri['subject3'].shape)\n",
    "        #print(true_fmri['subject2'].shape)\n",
    "        #print(true_fmri['subject3'].shape)\n",
    "\n",
    "    return pred_fmri, true_fmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e8c4a6-e68c-43a7-89d4-c9a05a2fce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_fmri, test_true_fmri = make_prediction(test_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e78d0784-b91e-4928-8b86-3788397916b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_voxels(pred_stories, top_perc=1):\n",
    "    result = {}\n",
    "\n",
    "    def voxelwise_corr(y_pred, y_true):\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "        corr = np.array([\n",
    "            pearsonr(y_pred[:, v], y_true[:, v])[0]\n",
    "            for v in range(y_true.shape[1])\n",
    "        ])\n",
    "        return corr\n",
    "    \n",
    "    for story in pred_stories:\n",
    "        story_dict = {}\n",
    "        test_pred_fmri, test_true_fmri = make_prediction([story])\n",
    "        for subj in fmri_data.keys():\n",
    "            corr = voxelwise_corr(test_pred_fmri[subj], test_true_fmri[subj])\n",
    "            thresh = np.percentile(corr, 100 - top_perc)\n",
    "            print(f\"{story} ({subj}): {thresh}\")\n",
    "            indices = np.where(corr >= thresh)[0]\n",
    "            story_dict[subj] = indices\n",
    "\n",
    "        result[story] = story_dict\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbd4bd5-19ab-408d-862a-9f415b3295b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "topv = top_voxels(test_stories[:2], top_perc=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d281d94-2110-48fe-9ee3-ecad9ee99a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write top-voxels to pickle\n",
    "with open('topv.pkl', 'wb') as f:\n",
    "    pickle.dump(topv, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dc5b69f-2f6b-43c8-9f5f-a52974a4dcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read top-voxels to pickle\n",
    "with open('topv.pkl', 'rb') as f:\n",
    "    topv = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40af57f-a18b-4775-81ac-0bb9916d9c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = \"subject3\"\n",
    "\n",
    "base_model = BertModel.from_pretrained(model_name).to(device).eval()\n",
    "if lora_model:\n",
    "    base_model = get_peft_model(base_model, config).to(device)\n",
    "    base_model.load_state_dict(lora_weights[subj])\n",
    "    base_model.eval()\n",
    "\n",
    "test_story = test_stories[1]\n",
    "\n",
    "\n",
    "test_features = forward_pass([test_story], base_model)\n",
    "train_features = forward_pass(train_stories[:5], base_model)\n",
    "selected_voxels = topv[test_story][subj]\n",
    "\n",
    "classifier = classifiers[subj]\n",
    "\n",
    "def wrapped_shap_model(X_numpy):\n",
    "    X_tensor = torch.tensor(X_numpy, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = classifier(X_tensor)\n",
    "        return output[:, selected_voxels].cpu().numpy()\n",
    "\n",
    "def wrapped_lime_model(X_numpy):\n",
    "    batch_size = 128\n",
    "    X_tensor = torch.tensor(X_numpy, dtype=torch.float32).to(device)\n",
    "    \n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, X_tensor.size(0), batch_size):\n",
    "            batch = X_tensor[i:i+batch_size].unsqueeze(1)\n",
    "            output = classifier(batch)\n",
    "            preds.append(output[:, 0, selected_voxels].cpu())\n",
    "    return torch.cat(preds, dim=0).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1feeea9b-ff50-49f1-bd03-0911a2716ba4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test = test_features.detach().cpu().numpy()\n",
    "X_train = train_features.detach().cpu().numpy()\n",
    "background = X_train.mean(axis=0, keepdims=True) # test value\n",
    "#background_all = np.vstack([\n",
    "#    train_features[story].detach().cpu().numpy() for story in train_stories[:5]\n",
    "#])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92abc1-6589-4032-aaab-b233fd02d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explainer = shap.KernelExplainer(wrapped_shap_model, background)\n",
    "shap_values = shap_explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5c10601-e107-49f9-bd17-7d104c7e0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explainer = LimeTabularExplainer(\n",
    "    training_data = background,\n",
    "    mode = \"regression\",\n",
    "    feature_names = [i for i in range(X_test.shape[1])],\n",
    "    discretize_continuous=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3348829c-573c-4d49-98ce-206caea89d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lime(chunk_i):\n",
    "    lime_values_chunk = np.zeros((num_features, num_voxels))\n",
    "    for voxel_i in range(num_voxels):\n",
    "        def voxel_predict_fn(x):\n",
    "            return wrapped_lime_model(x)[:, voxel_i]\n",
    "\n",
    "        explanation = lime_explainer.explain_instance(data_row=X_test[chunk_i], predict_fn=voxel_predict_fn, num_features=10)\n",
    "\n",
    "        for feature, weight in explanation.as_list():\n",
    "            lime_values_chunk[feature, voxel_i] = weight\n",
    "\n",
    "    print(chunk_i, end=\" \")\n",
    "    return lime_values_chunk\n",
    "\n",
    "num_chunks = X_test.shape[0]\n",
    "num_features = X_test.shape[1]\n",
    "num_voxels = len(selected_voxels)\n",
    "\n",
    "lime_values_list = [\n",
    "    compute_lime(chunk_i)\n",
    "    for chunk_i in range(num_chunks)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e4571ae-8ee2-47c8-97f8-d3c063b5551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results into the correct shape\n",
    "lime_values = np.zeros((num_chunks, num_features, num_voxels))\n",
    "for i, chunk_i in enumerate(range(num_chunks)):\n",
    "    lime_values[chunk_i, :, :] = lime_values_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a4980cb-7cb8-4a30-8447-5930f1a0de8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write LIME values to pickle\n",
    "with open('lime.pkl', 'wb') as f:\n",
    "    pickle.dump(lime_values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5459e6a5-cc69-4f41-a74f-9886466c8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write SHAP values to pickle\n",
    "with open('shap.pkl', 'wb') as f:\n",
    "    pickle.dump(shap_values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa3d0ecc-0c10-4e48-86f9-4a187038265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read LIME values from pickle\n",
    "with open('lime.pkl', 'rb') as f:\n",
    "    lime_values = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34880979-00f0-4cfd-99b4-50dc6b4c0a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read SHAP values from pickle\n",
    "with open('shap.pkl', 'rb') as f:\n",
    "    shap_values = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
