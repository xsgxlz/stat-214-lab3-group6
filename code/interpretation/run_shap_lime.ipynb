{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ac60aa-13f6-468d-b029-96f4a96323bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# add local paths for module imports\n",
    "sys.path.append('code')\n",
    "sys.path.append(\"/jet/home/azhang19/stat 214/stat-214-lab3-group6/code\")\n",
    "\n",
    "# import project-specific utilities\n",
    "from BERT.data import TextDataset\n",
    "from finetune_bert_utils import (\n",
    "    get_sliding_window_embeddings,\n",
    "    aggregate_embeddings,\n",
    "    downsample_word_vectors_torch,\n",
    "    load_fmri_data,\n",
    "    get_fmri_data\n",
    ")\n",
    "\n",
    "# set computation precision and device\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# set base path for data access\n",
    "data_path = '/ocean/projects/mth240012p/shared/data'\n",
    "\n",
    "# interpretatiion-specific packages\n",
    "import shap\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b793fc1-78d2-4990-b4bd-dc87fea3ee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word sequences for each story\n",
    "with open(f'{data_path}/raw_text.pkl', 'rb') as file:\n",
    "    wordseqs = pickle.load(file)  # {story_id: WordSequenceObject}\n",
    "\n",
    "# extract story IDs and split them into train/val/test sets\n",
    "stories = [i[:-4] for i in os.listdir(f'{data_path}/subject2')]\n",
    "train_stories, temp_stories = train_test_split(stories, train_size=0.6, random_state=214)\n",
    "val_stories, test_stories = train_test_split(temp_stories, train_size=0.5, random_state=214)\n",
    "\n",
    "# mapping from story names to index\n",
    "story_name_to_idx = {story: i for i, story in enumerate(stories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb8f0a4c-f5ad-4013-9536-44b07b5e7c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer and base BERT model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "base_model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48efaf95-1caa-43ce-ae27-3141fc8092a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset for training\n",
    "train_text = [\" \".join(wordseqs[i].data).strip() for i in train_stories]\n",
    "train_dataset = TextDataset(train_text, tokenizer, max_len=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "551c6667-3b97-4a5e-a1c2-08d4eba4ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trimming indices to exclude padding\n",
    "trim_range = (5, -10)\n",
    "\n",
    "# initialize tokenizer (is this used?)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "embeddings = {}\n",
    "\n",
    "# tokenize full text of all stories\n",
    "texts = []\n",
    "for story in stories:\n",
    "    words = wordseqs[story].data\n",
    "    texts.append(\" \".join(words).strip())\n",
    "    tokens = tokenizer(words, add_special_tokens=False, truncation=False, max_length=sys.maxsize)['input_ids']\n",
    "    token_per_word = [len(i) for i in tokens]\n",
    "    \n",
    "tokenlized_stories = tokenizer(texts, add_special_tokens=False, padding=\"longest\", truncation=False, max_length=sys.maxsize,\n",
    "                               return_token_type_ids=False, return_tensors=\"pt\")\n",
    "input_ids = tokenlized_stories[\"input_ids\"].to(device)\n",
    "attention_mask = tokenlized_stories[\"attention_mask\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8e063ef-7bfb-4ec2-b211-f5a4a6151597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding and feature extraction\n",
    "def forward_pass(current_stories, base_model):\n",
    "    \"\"\"\n",
    "    Extract downsampled and aggregated word embeddings for a set of stories.\n",
    "\n",
    "    Args:\n",
    "        current_stories (list of str): List of story IDs.\n",
    "        base_model (transformers.BertModel): Pretrained BERT model.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Aggregated feature representations.\n",
    "    \"\"\"\n",
    "    idx = torch.tensor([story_name_to_idx[story] for story in current_stories], device=input_ids.device)\n",
    "    selected_input_ids = input_ids[idx].to(input_ids.device)\n",
    "    selected_attention_mask = attention_mask[idx].to(attention_mask.device)\n",
    "    \n",
    "    #idx = [story_name_to_idx[story] for story in current_stories]\n",
    "    #embeddings = get_sliding_window_embeddings(base_model, input_ids[idx], attention_mask[idx])\n",
    "\n",
    "    print(base_model.device)\n",
    "    print(selected_input_ids.device)\n",
    "    print(selected_attention_mask.device)\n",
    "\n",
    "    embeddings = get_sliding_window_embeddings(base_model, selected_input_ids, selected_attention_mask)\n",
    "\n",
    "    features = {}\n",
    "    for i, story in enumerate(current_stories):\n",
    "        words = wordseqs[story].data\n",
    "        tokens = tokenizer(words, add_special_tokens=False, truncation=False, max_length=sys.maxsize)['input_ids']\n",
    "        token_per_word = [len(i) for i in tokens]\n",
    "        story_embeddings = embeddings[i]\n",
    "        word_embeddings = []\n",
    "        start = 0\n",
    "        for i in token_per_word:\n",
    "            end = start + i\n",
    "            if i != 0:\n",
    "                word_embedding = story_embeddings[start:end].mean(dim=0)\n",
    "            else:\n",
    "                word_embedding = torch.zeros(story_embeddings.size(1), device=device)\n",
    "            word_embeddings.append(word_embedding)\n",
    "            start = end\n",
    "        \n",
    "        features[story] = torch.stack(word_embeddings)#.cpu().numpy()\n",
    "\n",
    "    features = downsample_word_vectors_torch(current_stories, features, wordseqs)\n",
    "    for story in current_stories:\n",
    "        features[story] = features[story][trim_range[0]:trim_range[1]]\n",
    "\n",
    "    aggregated_features = aggregate_embeddings(features, current_stories)\n",
    "    return aggregated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03d51975-4517-4f8f-957e-396bdf80a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fMRI data\n",
    "fmri_data = load_fmri_data(test_stories, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0486a29d-3e50-43f6-ae7a-52c278dbfcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config and classifier loading\n",
    "weight_decay = 1e-2\n",
    "lora_model = True\n",
    "lora_rank = 8\n",
    "\n",
    "if lora_model:\n",
    "    config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_rank * 2,\n",
    "    target_modules=['query', 'value'],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.FEATURE_EXTRACTION\n",
    "    )\n",
    "    ckpt = torch.load(f'/jet/home/azhang19/stat 214/stat-214-lab3-group6/code/classifier_ckpt/best_lora_wd{weight_decay}_r{lora_rank}.pth', weights_only=False)\n",
    "    # was /ocean/projects/mth240012p/azhang19/lab3/classifier_ckpts\n",
    "    classifiers = {i: ckpt[i]['classifier_module'] for i in ckpt.keys()}\n",
    "    lora_weights = {i: ckpt[i]['lora_state_dict'] for i in ckpt.keys()}\n",
    "else:\n",
    "    classifiers = torch.load(f'/jet/home/azhang19/stat 214/stat-214-lab3-group6/code/classifier_ckpt/best_classifiers{weight_decay}.pth', weights_only=False)\n",
    "    # was /ocean/projects/mth240012p/azhang19/lab3/classifier_ckpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9bda946-12da-4ac5-9c7e-a2e081a576be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on fMRI data\n",
    "def make_prediction(pred_stories):\n",
    "    \"\"\"\n",
    "    Generate fMRI predictions and return ground-truth values.\n",
    "\n",
    "    Args:\n",
    "        pred_stories (list of str): List of story IDs to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        tuple of dict: (predicted_fmri, true_fmri) keyed by subject ID.\n",
    "    \"\"\"\n",
    "    with torch.inference_mode():\n",
    "        pred_fmri = {}\n",
    "        true_fmri = {}\n",
    "        for subj in fmri_data.keys():\n",
    "            base_model = BertModel.from_pretrained(model_name).to(device).eval()\n",
    "            if lora_model:\n",
    "                base_model = get_peft_model(base_model, config).to(device)\n",
    "                base_model.load_state_dict(lora_weights[subj])\n",
    "                base_model.eval()\n",
    "        \n",
    "            features = forward_pass(pred_stories, base_model)\n",
    "            # added base_model as parameter to avoid cpu/cuda device mismatch\n",
    "            pred_fmri[subj] = classifiers[subj](features)\n",
    "            true_fmri[subj] = get_fmri_data(pred_stories, fmri_data)[subj]\n",
    "        #print(pred_fmri['subject2'].shape)\n",
    "        #print(pred_fmri['subject3'].shape)\n",
    "        #print(true_fmri['subject2'].shape)\n",
    "        #print(true_fmri['subject3'].shape)\n",
    "\n",
    "    return pred_fmri, true_fmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e8c4a6-e68c-43a7-89d4-c9a05a2fce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test make_predictions()\n",
    "test_pred_fmri, test_true_fmri = make_prediction(test_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e78d0784-b91e-4928-8b86-3788397916b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify top correlated voxels\n",
    "def top_voxels(pred_stories, top_perc=1):\n",
    "    \"\"\"\n",
    "    Select top-percentile voxels based on prediction correlation.\n",
    "\n",
    "    Args:\n",
    "        pred_stories (list of str): Stories to evaluate.\n",
    "        top_perc (float): Top percentile of voxels to keep.\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping from story → subject → voxel indices.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    def voxelwise_corr(y_pred, y_true):\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "\n",
    "        corr = np.array([\n",
    "            pearsonr(y_pred[:, v], y_true[:, v])[0]\n",
    "            for v in range(y_true.shape[1])\n",
    "        ])\n",
    "        return corr\n",
    "    \n",
    "    for story in pred_stories:\n",
    "        story_dict = {}\n",
    "        test_pred_fmri, test_true_fmri = make_prediction([story])\n",
    "        for subj in fmri_data.keys():\n",
    "            corr = voxelwise_corr(test_pred_fmri[subj], test_true_fmri[subj])\n",
    "            thresh = np.percentile(corr, 100 - top_perc)\n",
    "            print(f\"{story} ({subj}): {thresh}\")\n",
    "            indices = np.where(corr >= thresh)[0]\n",
    "            story_dict[subj] = indices\n",
    "\n",
    "        result[story] = story_dict\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbd4bd5-19ab-408d-862a-9f415b3295b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "topv = top_voxels(test_stories[:2], top_perc=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d281d94-2110-48fe-9ee3-ecad9ee99a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save top-voxels to pickle\n",
    "with open('topv.pkl', 'wb') as f:\n",
    "    pickle.dump(topv, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dc5b69f-2f6b-43c8-9f5f-a52974a4dcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load top-voxels from pickle\n",
    "with open('topv.pkl', 'rb') as f:\n",
    "    topv = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40af57f-a18b-4775-81ac-0bb9916d9c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare features and classifiers for interpretation\n",
    "subj = \"subject3\" # \"subject2\" or \"subject3\"\n",
    "base_model = BertModel.from_pretrained(model_name).to(device).eval()\n",
    "if lora_model:\n",
    "    base_model = get_peft_model(base_model, config).to(device)\n",
    "    base_model.load_state_dict(lora_weights[subj])\n",
    "    base_model.eval()\n",
    "\n",
    "test_story = test_stories[1] # adjust index\n",
    "test_features = forward_pass([test_story], base_model)\n",
    "train_features = forward_pass(train_stories[:5], base_model)\n",
    "selected_voxels = topv[test_story][subj]\n",
    "classifier = classifiers[subj]\n",
    "\n",
    "# wrap models for SHAP and LIME\n",
    "def wrapped_shap_model(X_numpy):\n",
    "    \"\"\"\n",
    "    SHAP-compatible model wrapper.\n",
    "\n",
    "    Args:\n",
    "        X_numpy (np.ndarray): Input features.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Model predictions for selected voxels.\n",
    "    \"\"\"\n",
    "    X_tensor = torch.tensor(X_numpy, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = classifier(X_tensor)\n",
    "        return output[:, selected_voxels].cpu().numpy()\n",
    "\n",
    "def wrapped_lime_model(X_numpy):\n",
    "    \"\"\"\n",
    "    LIME-compatible model wrapper.\n",
    "\n",
    "    Args:\n",
    "        X_numpy (np.ndarray): Input features.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Model predictions for selected voxels.\n",
    "    \"\"\"\n",
    "    batch_size = 128\n",
    "    X_tensor = torch.tensor(X_numpy, dtype=torch.float32).to(device)\n",
    "    \n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, X_tensor.size(0), batch_size):\n",
    "            batch = X_tensor[i:i+batch_size].unsqueeze(1)\n",
    "            output = classifier(batch)\n",
    "            preds.append(output[:, 0, selected_voxels].cpu())\n",
    "    return torch.cat(preds, dim=0).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1feeea9b-ff50-49f1-bd03-0911a2716ba4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# feature matrices for explanation\n",
    "X_test = test_features.detach().cpu().numpy()\n",
    "X_train = train_features.detach().cpu().numpy()\n",
    "background = X_train.mean(axis=0, keepdims=True) # test value\n",
    "#background_all = np.vstack([\n",
    "#    train_features[story].detach().cpu().numpy() for story in train_stories[:5]\n",
    "#])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92abc1-6589-4032-aaab-b233fd02d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize SHAP KernelExplainer using a wrapped prediction model and background dataset\n",
    "# the model should return outputs shaped (n_samples, n_voxels)\n",
    "shap_explainer = shap.KernelExplainer(wrapped_shap_model, background)\n",
    "\n",
    "# compute SHAP values for each sample in the test set\n",
    "# output is a list of arrays, one per voxel: each with shape (n_samples, n_features)\n",
    "shap_values = shap_explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5c10601-e107-49f9-bd17-7d104c7e0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize LIME TabularExplainer with training background\n",
    "# we use regression mode since the model returns continuous values\n",
    "lime_explainer = LimeTabularExplainer(\n",
    "    training_data = background,\n",
    "    mode = \"regression\",\n",
    "    feature_names = [i for i in range(X_test.shape[1])],\n",
    "    discretize_continuous=False # keep features continuous instead of binning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3348829c-573c-4d49-98ce-206caea89d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lime(chunk_i):\n",
    "    \"\"\"\n",
    "    Computes LIME explanations for a single instance in the test set across all output voxels.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    chunk_i : int\n",
    "        Index of the test sample in X_test to explain.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    lime_values_chunk : np.ndarray\n",
    "        A (num_features, num_voxels) matrix where each [feature, voxel] entry is the LIME weight\n",
    "        for that feature when predicting that voxel for the given test sample.\n",
    "    \"\"\"\n",
    "    lime_values_chunk = np.zeros((num_features, num_voxels))\n",
    "    for voxel_i in range(num_voxels):\n",
    "        def voxel_predict_fn(x):\n",
    "            # define a wrapper to extract predictions for a specific voxel\n",
    "            return wrapped_lime_model(x)[:, voxel_i]\n",
    "\n",
    "        # run LIME for this voxel and test sample (top 10 most important features only)\n",
    "        explanation = lime_explainer.explain_instance(data_row=X_test[chunk_i], predict_fn=voxel_predict_fn, num_features=10)\n",
    "\n",
    "        # store weights for returned feature indices\n",
    "        for feature, weight in explanation.as_list():\n",
    "            lime_values_chunk[feature, voxel_i] = weight\n",
    "\n",
    "    print(chunk_i, end=\" \") # progress indicator\n",
    "    return lime_values_chunk\n",
    "\n",
    "# get shape information\n",
    "num_chunks = X_test.shape[0]       # number of test samples\n",
    "num_features = X_test.shape[1]     # number of input features\n",
    "num_voxels = len(selected_voxels)  # number of model outputs (voxels)\n",
    "\n",
    "# compute LIME values for each test sample\n",
    "lime_values_list = [\n",
    "    compute_lime(chunk_i)\n",
    "    for chunk_i in range(num_chunks)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e4571ae-8ee2-47c8-97f8-d3c063b5551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack results into a 3D array: (n_samples, n_features, n_voxels)\n",
    "lime_values = np.zeros((num_chunks, num_features, num_voxels))\n",
    "for i, chunk_i in enumerate(range(num_chunks)):\n",
    "    lime_values[chunk_i, :, :] = lime_values_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a4980cb-7cb8-4a30-8447-5930f1a0de8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save LIME values to pickle\n",
    "with open('lime.pkl', 'wb') as f:\n",
    "    pickle.dump(lime_values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5459e6a5-cc69-4f41-a74f-9886466c8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save SHAP values to pickle\n",
    "with open('shap.pkl', 'wb') as f:\n",
    "    pickle.dump(shap_values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa3d0ecc-0c10-4e48-86f9-4a187038265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load LIME values from pickle\n",
    "with open('lime.pkl', 'rb') as f:\n",
    "    lime_values = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34880979-00f0-4cfd-99b4-50dc6b4c0a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SHAP values from pickle\n",
    "with open('shap.pkl', 'rb') as f:\n",
    "    shap_values = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
