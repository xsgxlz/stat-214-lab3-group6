{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('code')\n",
    "sys.path.append(\"/jet/home/azhang19/stat 214/stat-214-lab3-group6/code\")\n",
    "\n",
    "from BERT.data import TextDataset\n",
    "from finetune_bert_utils import get_sliding_window_embeddings, aggregate_embeddings, downsample_word_vectors_torch, load_fmri_data, get_fmri_data\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define the base path for data access\n",
    "data_path = '/ocean/projects/mth240012p/shared/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_85102/1482417774.py:3: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  wordseqs = pickle.load(file) # wordseqs is expected to be a dictionary: {story_id: WordSequenceObject}\n"
     ]
    }
   ],
   "source": [
    "# %% Load preprocessed word sequences (likely includes words and their timings)\n",
    "with open(f'{data_path}/raw_text.pkl', 'rb') as file:\n",
    "    wordseqs = pickle.load(file) # wordseqs is expected to be a dictionary: {story_id: WordSequenceObject}\n",
    "\n",
    "# %% Get list of story identifiers and split into training and testing sets\n",
    "# Assumes story data for 'subject2' exists and filenames are story IDs + '.npy'\n",
    "stories = [i[:-4] for i in os.listdir(f'{data_path}/subject2')] # Extract story IDs from filenames\n",
    "# Split stories into train and test sets with a fixed random state for reproducibility\n",
    "\n",
    "\n",
    "# First, use 60% for training and 40% for the remaining data.\n",
    "train_stories, temp_stories = train_test_split(stories, train_size=0.6, random_state=214)\n",
    "# Then split the remaining 40% equally to get 20% validation and 20% test.\n",
    "val_stories, test_stories = train_test_split(temp_stories, train_size=0.5, random_state=214)\n",
    "\n",
    "story_name_to_idx = {story: i for i, story in enumerate(stories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "original_base_model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_rank = 8\n",
    "lora_alpha = lora_rank * 2\n",
    "lora_dropout = 0.1\n",
    "\n",
    "target_modules_bert = [\n",
    "    \"query\", \"value\",\n",
    "    # \"key\",\n",
    "    # \"dense\"\n",
    "]\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=target_modules_bert,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.FEATURE_EXTRACTION\n",
    ")\n",
    "base_model = get_peft_model(original_base_model, config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [\" \".join(wordseqs[i].data).strip() for i in train_stories]\n",
    "train_dataset = TextDataset(train_text, tokenizer, max_len=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_range = (5, -10)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "texts = []\n",
    "\n",
    "for story in stories:\n",
    "    words = wordseqs[story].data\n",
    "    texts.append(\" \".join(words).strip())\n",
    "    tokens = tokenizer(words, add_special_tokens=False, truncation=False, max_length=sys.maxsize)['input_ids']\n",
    "    token_per_word = [len(i) for i in tokens]\n",
    "tokenlized_stories = tokenizer(texts, add_special_tokens=False, padding=\"longest\", truncation=False, max_length=sys.maxsize,\n",
    "                               return_token_type_ids=False, return_tensors=\"pt\")\n",
    "input_ids = tokenlized_stories[\"input_ids\"].to(device)\n",
    "attention_mask = tokenlized_stories[\"attention_mask\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(current_stories):\n",
    "    idx = [story_name_to_idx[story] for story in current_stories]\n",
    "    embeddings = get_sliding_window_embeddings(base_model, input_ids[idx], attention_mask[idx])\n",
    "\n",
    "    features = {}\n",
    "    for i, story in enumerate(current_stories):\n",
    "        words = wordseqs[story].data\n",
    "        tokens = tokenizer(words, add_special_tokens=False, truncation=False, max_length=sys.maxsize)['input_ids']\n",
    "        token_per_word = [len(i) for i in tokens]\n",
    "        story_embeddings = embeddings[i]\n",
    "        word_embeddings = []\n",
    "        start = 0\n",
    "        for i in token_per_word:\n",
    "            end = start + i\n",
    "            if i != 0:\n",
    "                word_embedding = story_embeddings[start:end].mean(dim=0)\n",
    "            else:\n",
    "                word_embedding = torch.zeros(story_embeddings.size(1), device=device)\n",
    "            word_embeddings.append(word_embedding)\n",
    "            start = end\n",
    "        \n",
    "        features[story] = torch.stack(word_embeddings)#.cpu().numpy()\n",
    "\n",
    "    features = downsample_word_vectors_torch(current_stories, features, wordseqs)\n",
    "    for story in current_stories:\n",
    "        features[story] = features[story][trim_range[0]:trim_range[1]]\n",
    "\n",
    "    aggregated_features = aggregate_embeddings(features, current_stories)\n",
    "    return aggregated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_data = load_fmri_data(stories, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No matter training or validation\n",
    "def get_loss(classifiers, sample_stories):\n",
    "    features = forward_pass(sample_stories)\n",
    "    pred_fmri = {}\n",
    "    loss = []\n",
    "    current_fmri = get_fmri_data(sample_stories, fmri_data)\n",
    "    for subj in fmri_data.keys():\n",
    "        pred_fmri[subj] = classifiers[subj](features)\n",
    "        obj = torch.from_numpy(current_fmri[subj]).float().to(device)\n",
    "        # Handle NaN values in obj\n",
    "        obj = torch.nan_to_num(obj, nan=0.0)\n",
    "        loss.append(nn.functional.mse_loss(pred_fmri[subj], obj))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(classifiers, sample_stories):\n",
    "    base_model.train()\n",
    "    loss = get_loss(classifiers, sample_stories)\n",
    "    loss_for_backprop = loss[0] + loss[1]\n",
    "\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss_for_backprop.backward()\n",
    "    optim.step()\n",
    "    return loss_for_backprop.item(), loss[0].item(), loss[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "loss_record = np.zeros((epochs, 2, 2)) # [epoch, subject_idx (0 for S2, 1 for S3), metric (0 for train, 1 for val)]\n",
    "\n",
    "best_val_loss_subject2 = float('inf')\n",
    "best_val_loss_subject3 = float('inf')\n",
    "\n",
    "best_classifier_s2 = None\n",
    "best_classifier_s3 = None\n",
    "best_lora_weights_for_s2 = None # LoRA weights when S2 validation was best\n",
    "best_lora_weights_for_s3 = None # LoRA weights when S3 validation was best\n",
    "\n",
    "\n",
    "#sample_stories = train_stories # Using all train stories per epoch\n",
    "def minibatch_iterator(story_list, batch_size):\n",
    "    stories_to_process = random.sample(story_list, len(story_list))\n",
    "\n",
    "    num_stories = len(stories_to_process)\n",
    "    for i in range(0, num_stories, batch_size):\n",
    "        yield stories_to_process[i : min(i + batch_size, num_stories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 1e-2\n",
    "#classifiers = {'subject2': nn.Linear(768, 94251, device=device), 'subject3': nn.Linear(768, 95556, device=device)}\n",
    "classifiers = torch.load(f'/ocean/projects/mth240012p/azhang19/lab3/classifier_ckpts/best_classifiers{weight_decay}.pth', weights_only=False)\n",
    "params_to_optimize = itertools.chain(\n",
    "    *[i.parameters() for i in classifiers.values()],\n",
    "    base_model.parameters()\n",
    ")\n",
    "optim = torch.optim.AdamW(params_to_optimize, lr=2e-3 / (len(train_stories) / 15), weight_decay=weight_decay, fused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Train S2: 0.9946, Train S3: 0.9925, Val S2: 1.0494, Val S3: 1.0488\n",
      "  New best Val Loss for Subject 2: 1.0494. Saved S2 classifier and current LoRA weights.\n",
      "  New best Val Loss for Subject 3: 1.0488. Saved S3 classifier and current LoRA weights.\n",
      "Epoch 2/2, Train S2: 0.9715, Train S3: 0.9691, Val S2: 1.0066, Val S3: 1.0045\n",
      "  New best Val Loss for Subject 2: 1.0066. Saved S2 classifier and current LoRA weights.\n",
      "  New best Val Loss for Subject 3: 1.0045. Saved S3 classifier and current LoRA weights.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss_subject2_train = 0\n",
    "    loss_subject3_train = 0\n",
    "    for sampled_stories in minibatch_iterator(train_stories, 15):\n",
    "        # train_step already sets base_model_peft.train()\n",
    "        _, loss_subject2_train_batch, loss_subject3_train_batch = train_step(classifiers, sampled_stories)\n",
    "        loss_subject2_train += loss_subject2_train_batch\n",
    "        loss_subject3_train += loss_subject3_train_batch\n",
    "    loss_subject2_train /= (len(train_stories) / 15)\n",
    "    loss_subject3_train /= (len(train_stories) / 15)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        base_model.eval()\n",
    "        val_losses = get_loss(classifiers, val_stories)\n",
    "        current_loss_subject2_val = val_losses[0].item()\n",
    "        current_loss_subject3_val = val_losses[1].item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train S2: {loss_subject2_train:.4f}, Train S3: {loss_subject3_train:.4f}, Val S2: {current_loss_subject2_val:.4f}, Val S3: {current_loss_subject3_val:.4f}\")\n",
    "    \n",
    "    loss_record[epoch, 0, 0] = loss_subject2_train\n",
    "    loss_record[epoch, 0, 1] = current_loss_subject2_val\n",
    "    loss_record[epoch, 1, 0] = loss_subject3_train\n",
    "    loss_record[epoch, 1, 1] = current_loss_subject3_val\n",
    "\n",
    "    # Check for Subject 2\n",
    "    if current_loss_subject2_val < best_val_loss_subject2:\n",
    "        best_val_loss_subject2 = current_loss_subject2_val\n",
    "        best_classifier_s2 = copy.deepcopy(classifiers['subject2'])\n",
    "        # Save the current LoRA weights that led to this best S2 val loss\n",
    "        best_lora_weights_for_s2 = copy.deepcopy(base_model.state_dict())\n",
    "        print(f\"  New best Val Loss for Subject 2: {best_val_loss_subject2:.4f}. Saved S2 classifier and current LoRA weights.\")\n",
    "\n",
    "    # Check for Subject 3\n",
    "    if current_loss_subject3_val < best_val_loss_subject3:\n",
    "        best_val_loss_subject3 = current_loss_subject3_val\n",
    "        best_classifier_s3 = copy.deepcopy(classifiers['subject3'])\n",
    "        # Save the current LoRA weights that led to this best S3 val loss\n",
    "        best_lora_weights_for_s3 = copy.deepcopy(base_model.state_dict())\n",
    "        print(f\"  New best Val Loss for Subject 3: {best_val_loss_subject3:.4f}. Saved S3 classifier and current LoRA weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj_s2 = {\n",
    "    'classifier_module': best_classifier_s2,\n",
    "    'lora_state_dict': best_lora_weights_for_s2,\n",
    "    'lora_config_params': { # Save LoRA config parameters for easier reloading\n",
    "        'r': config.r,\n",
    "        'lora_alpha': config.lora_alpha,\n",
    "        'target_modules': config.target_modules,\n",
    "        'lora_dropout': config.lora_dropout,\n",
    "        'bias': config.bias,\n",
    "        'task_type': str(config.task_type)\n",
    "    },\n",
    "    'base_model_name': model_name\n",
    "}\n",
    "\n",
    "save_obj_s3 = {\n",
    "    'classifier_module': best_classifier_s3,\n",
    "    'lora_state_dict': best_lora_weights_for_s3,\n",
    "    'val_loss': best_val_loss_subject3,\n",
    "    'lora_config_params': {\n",
    "        'r': config.r,\n",
    "        'lora_alpha': config.lora_alpha,\n",
    "        'target_modules': config.target_modules,\n",
    "        'lora_dropout': config.lora_dropout,\n",
    "        'bias': config.bias,\n",
    "        'task_type': str(config.task_type)\n",
    "    },\n",
    "    'base_model_name': model_name\n",
    "}\n",
    "filename = f'/ocean/projects/mth240012p/azhang19/lab3/classifier_ckpts/best_lora_wd{weight_decay}_r{lora_rank}.pth'\n",
    "torch.save({'subject2': save_obj_s2, 'subject3': save_obj_s3}, filename)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
