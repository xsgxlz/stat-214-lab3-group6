{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3ee63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('code')\n",
    "sys.path.append(\"/jet/home/azhang19/stat 214/stat-214-lab3-group6/code\")\n",
    "\n",
    "from BERT.data import TextDataset\n",
    "from BERT.train_encoder import Args, linear_warmup_cosine_decay_multiplicative\n",
    "from BERT.encoder import ModelArgs, Transformer\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define the base path for data access\n",
    "data_path = '/ocean/projects/mth240012p/shared/data' # Path where data files are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b009e2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_11173/46575173.py:3: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  wordseqs = pickle.load(file) # wordseqs is expected to be a dictionary: {story_id: WordSequenceObject}\n"
     ]
    }
   ],
   "source": [
    "# %% Load preprocessed word sequences (likely includes words and their timings)\n",
    "with open(f'{data_path}/raw_text.pkl', 'rb') as file:\n",
    "    wordseqs = pickle.load(file) # wordseqs is expected to be a dictionary: {story_id: WordSequenceObject}\n",
    "\n",
    "# %% Get list of story identifiers and split into training and testing sets\n",
    "# Assumes story data for 'subject2' exists and filenames are story IDs + '.npy'\n",
    "stories = [i[:-4] for i in os.listdir(f'{data_path}/subject2')] # Extract story IDs from filenames\n",
    "# Split stories into train and test sets with a fixed random state for reproducibility\n",
    "\n",
    "\n",
    "# First, use 60% for training and 40% for the remaining data.\n",
    "train_stories, test_stories = train_test_split(stories, train_size=0.75, random_state=214)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b38cef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "pretrained_word_embeddings = pretrained_bert.embeddings.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa2d7f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args Configuration:\n",
      "\n",
      "Training Parameters:\n",
      "  standard_lr:        3.2e-03\n",
      "  standard_epoch:     80000\n",
      "  standard_warmup_steps: 4000\n",
      "  batch_size:         4\n",
      "  min_lr:             1.0e-04\n",
      "  grad_clip_max_norm: 1.0\n",
      "  use_amp:            True\n",
      "  use_compile:        True\n",
      "\n",
      "Model Architecture Parameters:\n",
      "  dim:               32\n",
      "  n_layers:          2\n",
      "  n_heads:           4\n",
      "  hidden_dim:        112\n",
      "\n",
      "Save Path Parameters:\n",
      "  save_path:         \n",
      "  final_save_path:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the arguments\n",
    "##args = parse_args()\n",
    "args = Args(\n",
    "    # Training\n",
    "    standard_lr=3.16e-3,\n",
    "    standard_epoch=80000,\n",
    "    standard_warmup_steps=4000,\n",
    "    batch_size=4,\n",
    "    min_lr=1e-4,\n",
    "    grad_clip_max_norm=1.0,\n",
    "    use_amp=True,\n",
    "    use_compile=True,\n",
    "\n",
    "    # Model\n",
    "    dim=32,\n",
    "    n_layers=2,\n",
    "    n_heads=4,\n",
    "    hidden_dim=112,\n",
    "\n",
    "    # Save\n",
    "    save_path=\"\",\n",
    "    final_save_path=\"\",\n",
    ")\n",
    "\n",
    "print(args, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "457f5bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_text = [\" \".join(wordseqs[i].data).strip() for i in train_stories]\n",
    "train_dataset = TextDataset(train_text, tokenizer, max_len=65536)\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size,\n",
    "                                         num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55b70a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_args = ModelArgs(\n",
    "    dim=args.dim,\n",
    "    n_layers=args.n_layers,\n",
    "    n_heads=args.n_heads,\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    vocab_size=pretrained_word_embeddings.num_embeddings,\n",
    "    norm_eps=1e-5,\n",
    "    rope_theta=500000,\n",
    "    max_seq_len=train_dataset.encodings['input_ids'].size(1),\n",
    ")\n",
    "\n",
    "model = Transformer(params=transformer_args, pre_train_embeddings=pretrained_word_embeddings).to(device).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "865fa2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derived Parameters:\n",
      "lr: 2.46875e-05\n",
      "warmup_steps: 512000\n",
      "epochs: 10240000\n",
      "grad_clip_max_norm: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "standard_lr = args.standard_lr / 512\n",
    "standard_epoch = args.standard_epoch * 512\n",
    "standard_warmup_steps = args.standard_warmup_steps * 512\n",
    "batch_size = args.batch_size\n",
    "\n",
    "lr = standard_lr * batch_size\n",
    "warmup_steps = standard_warmup_steps // batch_size\n",
    "epochs = standard_epoch // batch_size\n",
    "\n",
    "print(\"Derived Parameters:\")\n",
    "print(f\"lr: {lr}\")\n",
    "print(f\"warmup_steps: {warmup_steps}\")\n",
    "print(f\"epochs: {epochs}\")\n",
    "print(f\"grad_clip_max_norm: {args.grad_clip_max_norm}\", end=\"\\n\\n\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, fused=True)\n",
    "scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer,\n",
    "            lr_lambda=lambda step: linear_warmup_cosine_decay_multiplicative(step, warmup_steps, epochs, args.min_lr))\n",
    "\n",
    "scaler = torch.amp.GradScaler(device, enabled=args.use_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be84c39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3586, 30522])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "tokens, masks = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "model(tokens, attn_mask=masks).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b64fab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 1037, 3232,  ..., 2178, 5001,  102], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0][tokens[0] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fada070c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
