{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('code')\n",
    "sys.path.append(\"/jet/home/azhang19/stat 214/stat-214-lab3-group6/code\")\n",
    "\n",
    "from BERT.data import TextDataset\n",
    "from BERT.train_encoder import Args, linear_warmup_cosine_decay_multiplicative\n",
    "from BERT.encoder import ModelArgs, Transformer\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define the base path for data access\n",
    "data_path = '/ocean/projects/mth240012p/shared/data' # Path where data files are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b009e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Load preprocessed word sequences (likely includes words and their timings)\n",
    "with open(f'{data_path}/raw_text.pkl', 'rb') as file:\n",
    "    wordseqs = pickle.load(file) # wordseqs is expected to be a dictionary: {story_id: WordSequenceObject}\n",
    "\n",
    "# %% Get list of story identifiers and split into training and testing sets\n",
    "# Assumes story data for 'subject2' exists and filenames are story IDs + '.npy'\n",
    "stories = [i[:-4] for i in os.listdir(f'{data_path}/subject2')] # Extract story IDs from filenames\n",
    "# Split stories into train and test sets with a fixed random state for reproducibility\n",
    "\n",
    "\n",
    "# First, use 60% for training and 40% for the remaining data.\n",
    "train_stories, test_stories = train_test_split(stories, train_size=0.75, random_state=214)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38cef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "pretrained_word_embeddings = pretrained_bert.embeddings.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d7f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the arguments\n",
    "# args = parse_args()\n",
    "args = Args(\n",
    "    # Training\n",
    "    standard_lr=1e-3,\n",
    "    standard_epoch=1000,\n",
    "    standard_warmup_steps=50,\n",
    "    batch_size=25,\n",
    "    min_lr=1e-4,\n",
    "    grad_clip_max_norm=1.0,\n",
    "    use_amp=True,\n",
    "    use_compile=True,\n",
    "\n",
    "    # Model\n",
    "    dim=32,\n",
    "    n_layers=2,\n",
    "    n_heads=4,\n",
    "    hidden_dim=112,\n",
    "\n",
    "    # BERT parameters\n",
    "    mlm_prob = 0.15,\n",
    "\n",
    "    # Save\n",
    "    save_path=\"/jet/home/azhang19/stat 214/stat-214-lab3-group6/code/ckpts\",\n",
    ")\n",
    "\n",
    "print(args, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457f5bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_text = [\" \".join(wordseqs[i].data).strip() for i in train_stories]\n",
    "train_dataset = TextDataset(train_text, tokenizer, max_len=sys.maxsize) # No limitation. The longest sequence is not too long.\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size,\n",
    "                                         num_workers=0, pin_memory=True)\n",
    "mean_len = (train_dataset.encodings['input_ids'] != 0).sum(dim=1).float().mean().item()\n",
    "print(f\"Mean length across all training sequences: {mean_len:.2f} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b70a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_args = ModelArgs(\n",
    "    dim=args.dim,\n",
    "    n_layers=args.n_layers,\n",
    "    n_heads=args.n_heads,\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    vocab_size=pretrained_word_embeddings.num_embeddings,\n",
    "    norm_eps=1e-5,\n",
    "    rope_theta=500000,\n",
    "    max_seq_len=train_dataset.encodings['input_ids'].size(1),\n",
    ")\n",
    "\n",
    "model = Transformer(params=transformer_args, pre_train_embeddings=pretrained_word_embeddings).to(device).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865fa2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "batch_size = args.batch_size\n",
    "\n",
    "lr = args.standard_lr * batch_size / len(train_stories) # len(train_stories) is the reference batch size\n",
    "warmup_steps = args.standard_warmup_steps\n",
    "epochs = args.standard_epoch\n",
    "\n",
    "print(\"Derived Parameters:\")\n",
    "print(f\"lr: {lr}\")\n",
    "print(f\"warmup_steps: {warmup_steps}\")\n",
    "print(f\"epochs: {epochs}\")\n",
    "print(f\"grad_clip_max_norm: {args.grad_clip_max_norm}\", end=\"\\n\\n\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, fused=True)\n",
    "scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer,\n",
    "            lr_lambda=lambda step: linear_warmup_cosine_decay_multiplicative(step, warmup_steps, epochs, args.min_lr))\n",
    "\n",
    "scaler = torch.amp.GradScaler(device, enabled=args.use_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9bab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_tokens(input_ids, vocab_size, mask_token_id, pad_token_id, mlm_prob=0.15):\n",
    "    \"\"\"\n",
    "    Prepare masked tokens inputs/labels for masked language modeling.\n",
    "    (Corrected version with explicit device handling)\n",
    "    Args:\n",
    "        input_ids (Tensor): [batch_size, seq_len] tensor of token ids\n",
    "        vocab_size (int): total number of tokens in vocab\n",
    "        mask_token_id (int): token id used for [MASK]\n",
    "        pad_token_id (int): token id used for padding\n",
    "        mlm_prob (float): probability of masking a token\n",
    "    Returns:\n",
    "        masked_input_ids: tensor with some tokens replaced for MLM\n",
    "        masked_indices: boolean tensor indicating which tokens were masked\n",
    "    \"\"\"\n",
    "    device = input_ids.device  # Get device from input tensor\n",
    "\n",
    "    # Generate mask: which tokens to mask\n",
    "    probability_matrix = torch.full(input_ids.shape, mlm_prob, device=device)\n",
    "    special_tokens_mask = (input_ids == pad_token_id)\n",
    "    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "    # Ensure boolean tensor for masked_indices is created\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "\n",
    "    # Replace 80% of the masked tokens with [MASK]\n",
    "    # Ensure boolean tensor for indices_replaced is created\n",
    "    indices_replaced = torch.bernoulli(torch.full(input_ids.shape, 0.8, device=device)).bool() & masked_indices\n",
    "    # Clone input_ids before modifying to avoid modifying original if it's needed elsewhere\n",
    "    masked_input_ids = input_ids.clone()\n",
    "    masked_input_ids[indices_replaced] = mask_token_id\n",
    "\n",
    "    # Replace 10% of the masked tokens with random token\n",
    "    # Ensure boolean tensor for indices_random is created\n",
    "    indices_random = torch.bernoulli(torch.full(input_ids.shape, 0.5, device=device)).bool() & masked_indices & ~indices_replaced \n",
    "    random_tokens = torch.randint(low=0, high=vocab_size, size=input_ids.shape, dtype=torch.long, device=device)\n",
    "    masked_input_ids[indices_random] = random_tokens[indices_random]\n",
    "\n",
    "    # 10% remain unchanged (masked_indices & not replaced & not random)\n",
    "\n",
    "    return masked_input_ids, masked_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5bd05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_loss_fn(input_ids, logits, loss_mask):\n",
    "    '''\n",
    "    Implement BERT loss function\n",
    "    Args:\n",
    "        input_ids: Input IDs (batch_size, seq_len) int\n",
    "        logits: Model logits (batch_size, seq_len, vocab_size) float\n",
    "        loss_mask: Mask for whether to include the token in the loss (batch_size, seq_len) bool\n",
    "    Returns:\n",
    "        loss: Scalar cross-entropy loss float\n",
    "    '''\n",
    "    # get dimensions of logits tensor\n",
    "    batch_size, seq_len, vocab_size = logits.size()\n",
    "\n",
    "    # input dimension and type validation\n",
    "    assert input_ids.size() == (batch_size, seq_len), f\"input_ids: expected ({batch_size}, {seq_len}), got {tuple(input_ids.size())}\"\n",
    "    assert loss_mask.size() == (batch_size, seq_len), f\"loss_mask: expected ({batch_size}, {seq_len}), got {tuple(loss_mask.size())}\"\n",
    "    assert loss_mask.dtype == torch.bool, f\"loss_mask must be boolean, got {loss_mask.dtype}\"\n",
    "    \n",
    "    # flatten input tensors\n",
    "    logits = logits.view(-1, vocab_size) # to (batch_size * seq_len, vocab_size)\n",
    "    input_ids = input_ids.view(-1) # to (batch_size * seq_len)\n",
    "    loss_mask = loss_mask.view(-1) # to (batch_size * seq_len)\n",
    "\n",
    "    # use mask to filter only unknown tokens\n",
    "    # where loss_mask (bool): True -> include in loss \n",
    "    logits_masked = logits[loss_mask]\n",
    "    input_ids_masked = input_ids[loss_mask]\n",
    "\n",
    "    # compute cross-entropy on unnormalized logits and true class indices\n",
    "    # Use reduction='sum', normalize later\n",
    "    loss = torch.nn.functional.cross_entropy(logits_masked, input_ids_masked, reduction='sum')    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41cacb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(model, loss, optimizer, scaler, grad_clip_max_norm):\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip_max_norm)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33777b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, input_ids, masked_input_ids, loss_mask, atten_masks, mean_len, optimizer, scaler, args):\n",
    "    device = input_ids.device\n",
    "    batch_size = input_ids.size(0)\n",
    "    \n",
    "    with torch.autocast(device_type=device.type, dtype=torch.bfloat16, enabled=args.use_amp):\n",
    "        pred = model(masked_input_ids, attn_mask=atten_masks)\n",
    "        \n",
    "        loss = bert_loss_fn(input_ids, pred, loss_mask)\n",
    "\n",
    "        # Normalize loss. Make the weight of each token is the same and the scale is invariant to the batch size and mlm_prob\n",
    "        loss_for_backward = loss / (mean_len * batch_size * args.mlm_prob)\n",
    "\n",
    "    backward_pass(model, loss_for_backward, optimizer, scaler, args.grad_clip_max_norm)\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507d209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile(disable=not args.use_compile)\n",
    "def train_one_epoch(model, dataloader, mean_len, optimizer, scheduler, scaler, args):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        atten_masks = batch['attention_mask'].to(device)\n",
    "\n",
    "        masked_input_ids, loss_mask = mask_tokens(input_ids, pretrained_word_embeddings.num_embeddings,\n",
    "                                              tokenizer.mask_token_id, tokenizer.pad_token_id, args.mlm_prob)\n",
    "\n",
    "        loss = train_step(model, input_ids, masked_input_ids, loss_mask, atten_masks, mean_len, optimizer, scaler, args)\n",
    "        total_loss += loss\n",
    "    \n",
    "    scheduler.step()\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e93738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(args, epoch):\n",
    "    name = f\"dim{args.dim}_mlm{args.mlm_prob}_epoch{epoch}\"\n",
    "    return name\n",
    "\n",
    "def save_model(model, loss_record, name, args):\n",
    "    save_path = f\"{args.save_path}/{name}.pth\"\n",
    "    torch.save((model, loss_record), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ae0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_epoch = [0, 100, 200, 300, 400, 500, 600, 700, 800, 900]\n",
    "\n",
    "loss_record = np.zeros(epochs)\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "while epoch < epochs:\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    if epoch in ckpt_epoch:\n",
    "        name = get_name(args, epoch)\n",
    "        save_model(model, loss_record, name, args)\n",
    "    \n",
    "    loss_record[epoch] = train_one_epoch(model, dataloader, mean_len, optimizer, scheduler, scaler, args)\n",
    "    # Normalize loss with mean number of masked tokens\n",
    "    loss_record[epoch] = loss_record[epoch] / (len(train_stories) * mean_len * args.mlm_prob)\n",
    "\n",
    "    epoch = epoch + 1\n",
    "\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    print(f\"Loss: {loss_record[epoch-1]:.4f}\")\n",
    "    print(f\"Time: {time.time() - t0:.2f} seconds\", end=\"\\n\\n\")\n",
    "\n",
    "name = get_name(args, epoch)\n",
    "save_model(model, loss_record, name, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
