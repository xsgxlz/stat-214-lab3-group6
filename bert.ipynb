{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3ee63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('code')\n",
    "sys.path.append(\"/jet/home/azhang19/stat 214/stat-214-lab3-group6/code\")\n",
    "\n",
    "from BERT.data import TextDataset\n",
    "from BERT.train_encoder import Args, linear_warmup_cosine_decay_multiplicative\n",
    "from BERT.encoder import ModelArgs, Transformer\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define the base path for data access\n",
    "data_path = '/ocean/projects/mth240012p/shared/data' # Path where data files are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b009e2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_68014/46575173.py:3: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  wordseqs = pickle.load(file) # wordseqs is expected to be a dictionary: {story_id: WordSequenceObject}\n"
     ]
    }
   ],
   "source": [
    "# %% Load preprocessed word sequences (likely includes words and their timings)\n",
    "with open(f'{data_path}/raw_text.pkl', 'rb') as file:\n",
    "    wordseqs = pickle.load(file) # wordseqs is expected to be a dictionary: {story_id: WordSequenceObject}\n",
    "\n",
    "# %% Get list of story identifiers and split into training and testing sets\n",
    "# Assumes story data for 'subject2' exists and filenames are story IDs + '.npy'\n",
    "stories = [i[:-4] for i in os.listdir(f'{data_path}/subject2')] # Extract story IDs from filenames\n",
    "# Split stories into train and test sets with a fixed random state for reproducibility\n",
    "\n",
    "\n",
    "# First, use 60% for training and 40% for the remaining data.\n",
    "train_stories, test_stories = train_test_split(stories, train_size=0.75, random_state=214)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b38cef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "pretrained_word_embeddings = pretrained_bert.embeddings.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa2d7f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args Configuration:\n",
      "\n",
      "Training Parameters:\n",
      "  standard_lr:        1.0e-03\n",
      "  standard_epoch:     100\n",
      "  standard_warmup_steps: 10\n",
      "  batch_size:         10\n",
      "  min_lr:             1.0e-04\n",
      "  grad_clip_max_norm: 1.0\n",
      "  use_amp:            True\n",
      "  use_compile:        False\n",
      "\n",
      "Model Architecture Parameters:\n",
      "  dim:               32\n",
      "  n_layers:          2\n",
      "  n_heads:           4\n",
      "  hidden_dim:        112\n",
      "\n",
      "Save Path Parameters:\n",
      "  save_path:         \n",
      "  final_save_path:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the arguments\n",
    "# args = parse_args()\n",
    "args = Args(\n",
    "    # Training\n",
    "    standard_lr=1e-3,\n",
    "    standard_epoch=100,\n",
    "    standard_warmup_steps=10,\n",
    "    batch_size=10,\n",
    "    min_lr=1e-4,\n",
    "    grad_clip_max_norm=1.0,\n",
    "    use_amp=True,\n",
    "    use_compile=False,\n",
    "\n",
    "    # Model\n",
    "    dim=32,\n",
    "    n_layers=2,\n",
    "    n_heads=4,\n",
    "    hidden_dim=112,\n",
    "\n",
    "    # Save\n",
    "    save_path=\"\",\n",
    "    final_save_path=\"\",\n",
    ")\n",
    "\n",
    "print(args, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457f5bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length across all training sequences: 2057.68 tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_text = [\" \".join(wordseqs[i].data).strip() for i in train_stories]\n",
    "train_dataset = TextDataset(train_text, tokenizer, max_len=sys.maxsize) # No limitation\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size,\n",
    "                                         num_workers=0, pin_memory=True)\n",
    "mean_len = (train_dataset.encodings['input_ids'] != 0).sum(dim=1).float().mean().item()\n",
    "print(f\"Mean length across all training sequences: {mean_len:.2f} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55b70a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_args = ModelArgs(\n",
    "    dim=args.dim,\n",
    "    n_layers=args.n_layers,\n",
    "    n_heads=args.n_heads,\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    vocab_size=pretrained_word_embeddings.num_embeddings,\n",
    "    norm_eps=1e-5,\n",
    "    rope_theta=500000,\n",
    "    max_seq_len=train_dataset.encodings['input_ids'].size(1),\n",
    ")\n",
    "\n",
    "model = Transformer(params=transformer_args, pre_train_embeddings=pretrained_word_embeddings).to(device).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865fa2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derived Parameters:\n",
      "lr: 0.00013333333333333334\n",
      "warmup_steps: 75\n",
      "epochs: 750\n",
      "grad_clip_max_norm: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "batch_size = args.batch_size\n",
    "\n",
    "lr = args.standard_lr * batch_size / len(train_stories)\n",
    "warmup_steps = args.standard_warmup_steps\n",
    "epochs = args.standard_epoch\n",
    "\n",
    "print(\"Derived Parameters:\")\n",
    "print(f\"lr: {lr}\")\n",
    "print(f\"warmup_steps: {warmup_steps}\")\n",
    "print(f\"epochs: {epochs}\")\n",
    "print(f\"grad_clip_max_norm: {args.grad_clip_max_norm}\", end=\"\\n\\n\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, fused=True)\n",
    "scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer,\n",
    "            lr_lambda=lambda step: linear_warmup_cosine_decay_multiplicative(step, warmup_steps, epochs, args.min_lr))\n",
    "\n",
    "scaler = torch.amp.GradScaler(device, enabled=args.use_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d9bab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_tokens(input_ids, vocab_size, mask_token_id, pad_token_id, mlm_prob=0.15):\n",
    "    '''\n",
    "    Implement MLM masking\n",
    "    Args:\n",
    "        input_ids: Input IDs (batch_size, seq_len) int\n",
    "        vocab_size: Vocabulary size int\n",
    "        mask_token_id: Mask token ID int\n",
    "        pad_token_id: Pad token ID int\n",
    "        mlm_prob: Probability of masking float\n",
    "    Returns:\n",
    "        masked_input_ids: Masked input IDs (batch_size, seq_len) int\n",
    "        loss_mask: Loss mask (batch_size, seq_len) bool\n",
    "    '''\n",
    "    # Fake implementation\n",
    "    loss_mask = torch.randint(0, 2, input_ids.shape).bool()\n",
    "    masked_input_ids = input_ids.clone()\n",
    "    return masked_input_ids, loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e5bd05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_loss_fn(input_ids, logits, loss_mask):\n",
    "    '''\n",
    "    Implement BERT loss function\n",
    "    Args:\n",
    "        input_ids: Input IDs (batch_size, seq_len) int\n",
    "        logits: Model logits (batch_size, seq_len, vocab_size) float\n",
    "        loss_mask: Mask for whether to include the token in the loss (batch_size, seq_len) bool\n",
    "    Returns:\n",
    "        loss: Scalar cross-entropy loss float\n",
    "    '''\n",
    "    # get dimensions of logits tensor\n",
    "    batch_size, seq_len, vocab_size = logits.size()\n",
    "\n",
    "    # input dimension and type validation\n",
    "    assert input_ids.size() == (batch_size, seq_len), f\"input_ids: expected ({batch_size}, {seq_len}), got {tuple(input_ids.size())}\"\n",
    "    assert loss_mask.size() == (batch_size, seq_len), f\"loss_mask: expected ({batch_size}, {seq_len}), got {tuple(loss_mask.size())}\"\n",
    "    assert loss_mask.dtype == torch.bool, f\"loss_mask must be boolean, got {loss_mask.dtype}\"\n",
    "    \n",
    "    # flatten input tensors\n",
    "    logits = logits.view(-1, vocab_size) # to (batch_size * seq_len, vocab_size)\n",
    "    input_ids = input_ids.view(-1) # to (batch_size * seq_len)\n",
    "    loss_mask = loss_mask.view(-1) # to (batch_size * seq_len)\n",
    "\n",
    "    # use mask to filter only unknown tokens\n",
    "    # where loss_mask (bool): True -> include in loss \n",
    "    logits_masked = logits[loss_mask]\n",
    "    input_ids_masked = input_ids[loss_mask]\n",
    "\n",
    "    # compute cross-entropy on unnormalized logits and true class indices\n",
    "    loss = torch.nn.functional.cross_entropy(logits_masked, input_ids_masked, reduction='sum')    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be84c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))\n",
    "tokens, atten_masks = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "logits = model(tokens, attn_mask=atten_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41cacb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(model, loss, optimizer, scaler, scheduler, grad_clip_max_norm):\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip_max_norm)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33777b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, input_ids, masked_input_ids, loss_mask, atten_masks, mean_len, optimizer, scheduler, scaler, args):\n",
    "    device = input_ids.device\n",
    "    batch_size = input_ids.size(0)\n",
    "    \n",
    "    with torch.autocast(device_type=device.type, dtype=torch.bfloat16, enabled=args.use_amp):\n",
    "        pred = model(masked_input_ids, attn_mask=atten_masks)\n",
    "        \n",
    "        loss = bert_loss_fn(input_ids, pred, loss_mask)\n",
    "\n",
    "        loss_for_backward = loss / (mean_len * batch_size)\n",
    "\n",
    "    backward_pass(model, loss_for_backward, optimizer, scaler, scheduler, args.grad_clip_max_norm)\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507d209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile(disable=not args.use_compile)\n",
    "def train_one_epoch(model, dataloader, mean_len, optimizer, scheduler, scaler, args):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        atten_masks = batch['attention_mask'].to(device)\n",
    "\n",
    "        masked_input_ids, loss_mask = mask_tokens(input_ids, pretrained_word_embeddings.num_embeddings,\n",
    "                                              tokenizer.mask_token_id, tokenizer.pad_token_id)\n",
    "\n",
    "        loss = train_step(model, input_ids, masked_input_ids, loss_mask, atten_masks, mean_len, optimizer, scheduler, scaler, args)\n",
    "        total_loss += loss\n",
    "    \n",
    "    scheduler.step()\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ae0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Loss: 1370726.8125\n",
      "Time: 2.26 seconds\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 1348747.9375\n",
      "Time: 2.10 seconds\n",
      "\n",
      "Epoch: 3\n",
      "Loss: 1323750.6875\n",
      "Time: 2.10 seconds\n",
      "\n",
      "Epoch: 4\n",
      "Loss: 1296356.5625\n",
      "Time: 2.10 seconds\n",
      "\n",
      "Epoch: 5\n",
      "Loss: 1267956.4688\n",
      "Time: 2.10 seconds\n",
      "\n",
      "Epoch: 6\n",
      "Loss: 1237541.8750\n",
      "Time: 2.10 seconds\n",
      "\n",
      "Epoch: 7\n",
      "Loss: 1219054.0938\n",
      "Time: 2.10 seconds\n",
      "\n",
      "Epoch: 8\n",
      "Loss: 1199270.8750\n",
      "Time: 2.10 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_record = np.zeros(epochs)\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "while epoch < epochs:\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    loss_record[epoch] = train_one_epoch(model, dataloader, mean_len, optimizer, scheduler, scaler, args)\n",
    "\n",
    "    epoch = epoch + 1\n",
    "\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    print(f\"Loss: {loss_record[epoch-1]:.4f}\")\n",
    "    print(f\"Time: {time.time() - t0:.2f} seconds\", end=\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
