{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf44e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('code')\n",
    "sys.path.append(\"/jet/home/azhang19/stat 214/stat-214-lab3-group6/code\")\n",
    "\n",
    "from preprocessing import downsample_word_vectors, make_delayed\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define the base path for data access\n",
    "data_path = '/ocean/projects/mth240012p/shared/data' # Path where data files are stored\n",
    "ckpt_path = '/ocean/projects/mth240012p/azhang19/lab3/ckpts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3710769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Load preprocessed word sequences (likely includes words and their timings)\n",
    "with open(f'{data_path}/raw_text.pkl', 'rb') as file:\n",
    "    wordseqs = pickle.load(file) # wordseqs is expected to be a dictionary: {story_id: WordSequenceObject}\n",
    "\n",
    "# %% Get list of story identifiers and split into training and testing sets\n",
    "# Assumes story data for 'subject2' exists and filenames are story IDs + '.npy'\n",
    "stories = [i[:-4] for i in os.listdir(f'{data_path}/subject2')] # Extract story IDs from filenames\n",
    "# Split stories into train and test sets with a fixed random state for reproducibility\n",
    "\n",
    "\n",
    "# First, use 60% for training and 40% for the remaining data.\n",
    "train_stories, test_stories = train_test_split(stories, train_size=0.75, random_state=214)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90890b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 32 # [32, 64]\n",
    "mlm_prob = 0.15 # [0.1, 0.15, 0.2]\n",
    "epochs = 1000 # [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "filename = f\"dim{dim}_mlm{mlm_prob}_epoch{epochs}.pth\"\n",
    "model, record = torch.load(f\"{ckpt_path}/{filename}\", weights_only=False, map_location=device)\n",
    "model = model.to(device).eval()\n",
    "\n",
    "trim_range = (5, -10)\n",
    "delays = True\n",
    "delay_range = [0, 1, 2] # Includes 0, 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a2bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "embeddings = {}\n",
    "\n",
    "for story in stories:\n",
    "    words = wordseqs[story].data\n",
    "    tokens = tokenizer(words, add_special_tokens=False, truncation=False,max_length=sys.maxsize)['input_ids']\n",
    "    token_tensor = torch.tensor(sum(tokens, []), device=device).unsqueeze(0)\n",
    "    token_per_word = [len(i) for i in tokens]\n",
    "    with torch.inference_mode():\n",
    "        output = model(token_tensor,\n",
    "                    attn_mask=torch.ones_like(token_tensor, device=device, dtype=torch.bool), classification=False)\n",
    "        output = output.squeeze(0)\n",
    "    start = 0\n",
    "    word_embeddings = []\n",
    "    for i in token_per_word:\n",
    "        end = start + i\n",
    "        if i != 0:\n",
    "            word_embedding = output[start:end].mean(dim=0)\n",
    "        else:\n",
    "            word_embedding = torch.zeros(output.size(1), device=device)\n",
    "\n",
    "        word_embeddings.append(word_embedding)\n",
    "        start = end\n",
    "    embeddings[story] = torch.stack(word_embeddings).cpu().numpy()\n",
    "    \n",
    "story_embeddings = downsample_word_vectors(stories, embeddings, wordseqs)\n",
    "\n",
    "for story in stories:\n",
    "    if delays:\n",
    "        story_embeddings[story] = make_delayed(story_embeddings[story], delay_range)\n",
    "    story_embeddings[story] = story_embeddings[story][trim_range[0]:trim_range[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_embeddings(story_embeddings, stories):\n",
    "    \"\"\"\n",
    "    Aggregate the embeddings of the stories into a single tensor.\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    for story in stories:\n",
    "        all_embeddings.append(story_embeddings[story])\n",
    "    return np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "X_train = aggregate_embeddings(story_embeddings, train_stories)\n",
    "X_test = aggregate_embeddings(story_embeddings, test_stories)"
    "torch.save(torch.tensor(X_train), 'bert_X_train.pt')"
    "torch.save(torch.tensor(X_test), 'bert_X_test.pt')"

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd7b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define a function to load fMRI data for specified stories and subjects\n",
    "def load_fmri_data(stories, data_path):\n",
    "    \"\"\"\n",
    "    Loads fMRI data (.npy files) for given stories and subjects.\n",
    "\n",
    "    Args:\n",
    "        stories (list): List of story identifiers to load fMRI data for.\n",
    "        data_path (str): Base path where subject fMRI data is stored (e.g., data_path/subject_id/story_id.npy).\n",
    "\n",
    "    Returns:\n",
    "        dict: A nested dictionary `{subject_id: {story_id: fmri_data_array}}`.\n",
    "    \"\"\"\n",
    "    subjects = ['subject2', 'subject3'] # List of subjects to load data for\n",
    "    fmri_data = {} # Outer dictionary {subject: {story: data}}\n",
    "    for subject in subjects:\n",
    "        subject_dict = {} # Inner dictionary {story: data} for the current subject\n",
    "        for story in stories:\n",
    "            # Construct the full path to the fMRI data file\n",
    "            file_path = os.path.join(data_path, subject, f'{story}.npy')\n",
    "            # Load the NumPy array from the file\n",
    "            data = np.load(file_path)\n",
    "            subject_dict[story] = data # Store data in the inner dictionary\n",
    "        fmri_data[subject] = subject_dict # Store the subject's data dictionary\n",
    "    return fmri_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdc76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_data = load_fmri_data(stories, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84de15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fmri_data(stories, fmri_data):\n",
    "    \"\"\"\n",
    "    Concatenates fMRI data across specified stories for each subject.\n",
    "    Returns a dictionary: {subject: concatenated_fmri_array}.\n",
    "    \"\"\"\n",
    "    out_dict = {}\n",
    "    for subj in fmri_data.keys():\n",
    "        concatenated = np.concatenate(\n",
    "            [fmri_data[subj][st] for st in stories], axis=0\n",
    "        )\n",
    "        out_dict[subj] = concatenated\n",
    "    return out_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

   "cell_type": "code",
   "execution_count": null,
   "id": "84de15a8",
   "metadata": {},
   "outputs": [],
   "source": [

]
