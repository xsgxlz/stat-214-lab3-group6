{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "\n",
    "sys.path.append('code')\n",
    "import ridge_utils\n",
    "import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_1567/2788045516.py:2: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  wordseqs = pickle.load(file)\n"
     ]
    }
   ],
   "source": [
    "with open('data/raw_text.pkl', 'rb') as file:\n",
    "    wordseqs = pickle.load(file)\n",
    "\n",
    "stories = list(wordseqs.keys())\n",
    "train, test = train_test_split(stories, train_size=train_size, random_state=4193332621)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagofWords:\n",
    "    def __init__(self):\n",
    "        self.word_to_index = {}\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        for text in texts:\n",
    "            for word in text:\n",
    "                word = word.lower().strip()\n",
    "                if word not in self.word_to_index:\n",
    "                    self.word_to_index[word] = len(self.word_to_index)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, texts):\n",
    "        features = np.zeros((len(texts), self.vocab_size + 1))\n",
    "        for i, text in enumerate(texts):\n",
    "            for word in text:\n",
    "                word = word.lower().strip()\n",
    "                if word in self.word_to_index:\n",
    "                    features[i, self.word_to_index[word]] += 1\n",
    "                else:\n",
    "                    features[i, self.vocab_size] += 1\n",
    "        return features\n",
    "    \n",
    "    def transform_words(self, words):\n",
    "        tokens = []\n",
    "        for word in words:\n",
    "            word = word.lower().strip()\n",
    "            if word in self.word_to_index:\n",
    "                tokens.append(self.word_to_index[word])\n",
    "            else:\n",
    "                tokens.append(self.vocab_size)\n",
    "        # Convert to one-hot encoding\n",
    "        one_hot = np.zeros((len(tokens), self.vocab_size + 1))\n",
    "        one_hot[np.arange(len(tokens)), tokens] = 1\n",
    "        return one_hot\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.word_to_index)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'BagofWords(vocab_size={self.vocab_size})'\n",
    "    \n",
    "def trim_first_5_last_10(embeddings):\n",
    "    # Trim the first 5 and last 10 elements\n",
    "    for k, v in embeddings.items():\n",
    "        embeddings[k] = v[5:-10]\n",
    "    return embeddings\n",
    "    \n",
    "def bow_embed(stories, wordseqs, trimmed=True):\n",
    "    bow = BagofWords().fit([wordseqs[story].data for story in stories])\n",
    "    word_vectors = {}\n",
    "    for story in stories:\n",
    "        word_vectors[story] = bow.transform_words(wordseqs[story].data)\n",
    "    embeddings = preprocessing.downsample_word_vectors(stories, word_vectors, wordseqs)\n",
    "    if trimmed:\n",
    "        embeddings = trim_first_5_last_10(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "class embeddings_aggregator:\n",
    "    def __init__(self, delays=None, standardize=True):\n",
    "        self.delays = delays\n",
    "        self.standardize = standardize\n",
    "        self.scaler = sklearn.preprocessing.StandardScaler() if standardize else None\n",
    "\n",
    "    def _concatenate_embeddings(self, stories, embeddings):\n",
    "        all_embeddings = []\n",
    "        for story in stories:\n",
    "            all_embeddings.append(embeddings[story])\n",
    "        all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "        if self.delays:\n",
    "            all_embeddings = preprocessing.make_delayed(all_embeddings, self.delays)\n",
    "        return all_embeddings\n",
    "    \n",
    "    def fit(self, stories, embeddings):\n",
    "        if self.standardize:\n",
    "            all_embeddings = self._concatenate_embeddings(stories, embeddings)\n",
    "            self.scaler = sklearn.preprocessing.StandardScaler().fit(all_embeddings)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, stories, embeddings):\n",
    "        all_embeddings = self._concatenate_embeddings(stories, embeddings)\n",
    "        if self.standardize:\n",
    "            all_embeddings = self.scaler.transform(all_embeddings)\n",
    "        return all_embeddings\n",
    "    \n",
    "    def fit_transform(self, stories, embeddings):\n",
    "        all_embeddings = self._concatenate_embeddings(stories, embeddings)\n",
    "        if self.standardize:\n",
    "            all_embeddings = self.scaler.fit_transform(all_embeddings)\n",
    "        return all_embeddings\n",
    "\n",
    "train_embeddings = bow_embed(train, wordseqs)\n",
    "\n",
    "aggregator = embeddings_aggregator(delays=range(1, 5), standardize=True)\n",
    "train_X = aggregator.fit_transform(['adollshouse'], train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241, 44076)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.load('data/subject3/adollshouse.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = sklearn.linear_model.Ridge().fit(train_X, train_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
